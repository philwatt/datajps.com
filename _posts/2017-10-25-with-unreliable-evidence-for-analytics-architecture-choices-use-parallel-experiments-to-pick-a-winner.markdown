---
layout: post
title: With unreliable evidence for analytics architecture choices – use parallel
  experiments to pick a winner
date: '2017-10-25 11:02:00'
tags:
- software-engineering
- architecture
- a-b-testing
- devops
- analytics
---

* * *

We’ve all benefited over the last two decades from evidence based medicine. It’s really brought in an era of improved healthcare for all. There’s more to be done there of course – Ben Goldacre and his website [www.badscience,net](http://www.badscience.net) are highlighting this most effectively, but the trend is going the right way.

In the analytics world, I fear we are still in the age of ‘_eminence_ based practice’. Whoever has the most charismatic or persuasive argument tends to carry the day. There appears to be little incontrovertible evidence to support the current trendy methods. I’m not suggesting that all the current approaches are rubbish – far from it as there are some great ideas and approaches to consider. &nbsp;I am saying that often the choice to implement an architecture has more to do with feelings and emotions rather than hard headed, business based decisions.

For example, there’s lots of Open Source projects out there in Big Data land. And loads of proprietary tools as well. The choice often turns into a religious debate rather than a systemic examination of the pros and cons of the choices across the ecosystem. Neither Open Source nor proprietary tools are inherently bad or inherently good, but there is no effective decision tree, backed by evidence, to really support the decision process.

But what about analyst opinions for firms like Forrester, Gartner, Ovum, et al? They invest heavily in research and apply a lot of rigour. &nbsp;I certainly read their output an pay attention to it. But I also have a healthy level of scepticism. Analysts are often accused of ‘pay to play’ (although I have no real evidence for this) – the suggestion being that as a vendor you need to pay the analyst for inclusion in the analysis.

What I read from analysts often contradicts my own experience and that of many of my clients. I strongly suspect that there is some survivorship bias in their analysis. That is, the case studies and favourable reports of technologies are often from customers who have ‘succeeded’ (superficially anyway). It’s really hard to get anyone to speak candidly about their technology failures.

Rigourous analysis with open access to source data and analysis methods is fundamentally missing from the market today. What data points we can find are interpreted by consultants and applied locally (I’ve certainly done this for clients). &nbsp;But I fear a lot of money is being spent on science projects that ultimately disadvantage the enterprise.

Today’s modern enterprise must therefore do more experimentation. But that really means parallel projects running simultaneously that can fail fast – allowing the best, surviving project to fully implemented. That’s a different prospect from fully committing to the experiment based on a charismatic or influential individual. The evidence just isn’t there to support a single experiment – however confident you are.

